<!doctype html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="http://distill.pub/template.v2.js"></script>
    <style>
        <%=require("raw-loader!../static/style.css") %>
    </style>
</head>

<body>

    <d-front-matter>
        <script type="text/json">{
  "title": "A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features'",
  "description": "Six comments from the community and responses from the original authors",
  "authors": [
    {
      "author": "Logan Engstrom",
      "authorURL": "http://loganengstrom.com/",
      "affiliation": "MIT",
      "affiliationURL": ""
    },
    {
      "author": "Justin Gilmer",
      "authorURL": "https://www.linkedin.com/in/jmgilmer/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://ai.google/research/teams/brain"
    },
    {
      "author": "Gabriel Goh",
      "authorURL": "https://gabgoh.github.io/",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com/"
    },
    {
      "author": "Dan Hendrycks",
      "authorURL": "https://people.eecs.berkeley.edu/~hendrycks/",
      "affiliation": "UC Berkeley",
      "affiliationURL": ""
    },
    {
      "author": "Andrew Ilyas",
      "authorURL": "http://andrewilyas.com/",
      "affiliation": "MIT",
      "affiliationURL": ""
    },
    {
      "author": "Aleksander Madry",
      "authorURL": "https://people.csail.mit.edu/madry/",
      "affiliation": "MIT",
      "affiliationURL": ""
    },
    {
      "author": "Reiichiro Nakano",
      "authorURL": "https://reiinakano.com/",
      "affiliation": "",
      "affiliationURL": ""
    },
    {
      "author": "Preetum Nakkiran",
      "authorURL": "",
      "affiliation": "OpenAI & Harvard",
      "affiliationURL": ""
    },
    {
      "author": "Shibani Santurkar",
      "authorURL": "http://people.csail.mit.edu/shibani/",
      "affiliation": "MIT",
      "affiliationURL": ""
    },
    {
      "author": "Brandon Tran",
      "authorURL": "",
      "affiliation": "MIT",
      "affiliationURL": ""
    },
    {
      "author": "Dimitris Tsipras",
      "authorURL": "http://people.csail.mit.edu/tsipras/",
      "affiliation": "MIT",
      "affiliationURL": ""
    },
    {
      "author": "Eric Wallace",
      "authorURL": "http://www.ericswallace.com/",
      "affiliation": "Allen Institute for AI",
      "affiliationURL": ""
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
    </d-front-matter>

    <d-title>
        <h1>A Discussion of<br> <em>Adversarial Examples Are Not
                Bugs, They Are
                Features</em></h1>
        <!--<p>Ilya et al.'s paper was received with much interest by the research community.
            In an experimental attempt to advance the scientific
            dialogue,
            Distill is publishing a collection of commentary articles and clarifications developed in collaboration with
            the original paper's authors. </p>-->
    </d-title>

    <style>
        .authors-affiliations {
            display: none;
        }
    </style>

    <d-article>

        <p>
            On May 6th, Andrew Ilyas and colleagues <a href="http://gradientscience.org/adv/">published a paper</a>
            <d-cite key="ilyas2019adversarial"></d-cite> outlining two sets of experiments.
            Firstly, they showed that models trained on adversarial examples can transfer to real data,
            and secondly that models trained on a dataset derived from the representations of robust neural networks
            seem to inherit non-trivial robustness.
            They proposed an intriguing interpretation for their results:
            adversarial examples are due to "non-robust features" which are highly predictive but imperceptible to
            humans.
        </p>

        <p>
            The paper was received with intense interest and discussion
            on social media, mailing lists, and reading groups around the world.
            How should we interpret these experiments?
            Would they replicate?<d-footnote>
                Adversarial example research is particularly vulnerable to a certain kind of non-replication among
                disciplines of machine learning,
                because it requires researchers to play both attack and defense.
                It's easy for even very rigorous researchers to accidentally use a weak attack.
                However, as we'll see, Ilyas et al's results have held up to initial scrutiny.
            </d-footnote>
            And if non-robust features exist... what are they?
        </p>

        <p>
            To explore these questions, Distill decided to run an experimental "discussion article."<d-footnote>
                Running a discussion article is something Distill has wanted to try for several years.
                It was originally suggested to us by Ferenc Huszár, who writes many lovely discussions of papers on <a
                    href="https://www.inference.vc/">his blog</a>.
                <br><br>
                Why not just have everyone write private blog posts like Ferenc?
                Distill hopes that providing a more organized forum for many people to participate
                can give more researchers license to invest energy in discussing other's work
                and make sure there's an opportunity for all parties to comment and respond before the final version is
                published.
            </d-footnote>
            We invited a number of researchers
            to write comments on the paper and organized discussion and responses from the original authors.
        </p>

        <p>
            The Machine Learning community
            <a href="https://www.machinelearningdebates.com/program">sometimes</a>
            <a
                href="https://medium.com/syncedreview/cvpr-paper-controversy-ml-community-reviews-peer-review-79bf49eb0547">worries</a>
            that peer review isn't thorough enough.
            In contrast to this, we were struck by how deeply respondents engaged.
            Some respondents literally invested weeks in replicating results, running new experiments, and thinking
            deeply about the original paper.
            We also saw respondents update their views on non-robust features as they ran experiments — sometimes back
            and forth!
            The original authors similarly deeply engaged in discussing their results, clarifying misunderstandings, and
            even running new experiments in response to comments.
        </p>

        <p>
            We think this deep engagement and discussion is really exciting, and hope to experiment with more such
            discussion articles in the future.
        </p>

        <h2>Discussion Themes</h2>

        <p>
            <b>Clarifications</b>:
            Discussion between the respondents and original authors was able
            to surface several misunderstandings or opportunities to sharpen claims.
            The original authors summarize this in their rebuttal.
        </p>

        <p>
            <b>Successful Replication</b>:
            Respondents successfully reproduced many of the experiments in Ilyas et al <d-cite
                key="ilyas2019adversarial"></d-cite> and had no unsuccessful replication attempts.
            This was significantly facilitated by the release of code, models, and datasets by the original authors.
            Gabriel Goh and Preetum Nakkiran both independently reimplemented and replicated
            the non-robust dataset experiments.<d-footnote>
                Preetum reproduced the $\widehat{\mathcal{D}}_{det}$ non-robust dataset experiment as described in the
                paper, for $L_\infty$ and $L_2$ attacks.
                <br>
                Gabriel repproduced both $\widehat{\mathcal{D}}_{det}$ and $\widehat{\mathcal{D}}_{rand}$ for $L_2$
                attacks.
            </d-footnote>
            Preetum also replicated part of the robust dataset experiment by
            training models on the provided robust dataset and finding that they seemed non-trivially robust.
            It seems epistemically notable that both Preetum and Gabriel were initially skeptical.
            Preetum emphasizes that he found it easy to make the phenomenon work and that it was robust to many variants
            and hyperparameters he tried.
        </p>

        <p>
            <b>Exploring the Boundaries of Non-Robust Transfer</b>:
            Three of the comments focused on variants of the "non-robust dataset" experiment,
            where training on adversarial examples transfers to real data.
            When, how, and why does it happen?
            Gabriel Goh explores an alternative mechanism for the results,
            Preetum Nakkiran shows a special construction where it doesn't happen,
            and Eric Wallace shows that transfer can happen for other kinds of incorrectly labeled data.
        </p>

        <p>
            <b>Properties of Robust and Non-Robust Features</b>:
            The other three comments focused on the properties of robust and non-robust models.
            Gabriel Goh explores what non-robust features might look like in the case of linear models,
            while Dan Hendrycks and Justin Gilmer discuss how the results relate to the broader problem of robustness to
            distribution shift,
            and Reiichiro Nakano explores the qualitative differences of robust models in the context of style transfer.
        </p>



        <h2>Comments</h2>
        <p>
            Distill collected six comments on the original paper.
            They are presented in alphabetical order by the author's last name,
            with brief summaries of each comment and the corresponding response from the original authors.
        </p>

        <div id="commentaries" class="articles">
            <article id="response-1">
                <div class=icon></div>
                <header>
                    <h3>
                        <a href="response-1/">Adversarial Example Researchers Need to Expand What is Meant by
                            "Robustness"</a>
                    </h3>
                    <div class="authors-affiliations grid">
                        <h3>Authors</h3>
                        <h3>Affiliations</h3>
                        <p class="author">
                            <a class="name" href="https://www.linkedin.com/in/jmgilmer">Justin Gilmer</a>
                        </p>
                        <p class="affiliation">
                            <a class="affiliation" href="https://g.co/brain">Google Brain Team</a>
                        </p>
                        <p class="author">
                            <a class="name" href="https://people.eecs.berkeley.edu/~hendrycks/">Dan Hendrycks</a>
                        </p>
                        <p class="affiliation">
                            <a class="affiliation" href="https://www.berkeley.edu/">UC Berkeley</a>
                        </p>
                    </div>
                </header>
                <p class="summary">
                    Justin and Dan discuss "non-robust features" as a special case
                    of models being non-robust because they latch on to superficial correlations,
                    a view often found in the distributional robustness literature.
                    As an example, they discuss recent analysis of how neural networks behave in frequency space.
                    They emphasize we should think about a broader notion of robustness.
                    <a class="continue" href="response-1/">
                        Read Full Article
                    </a>
                </p>
                <div class="rebuttal">
                    <h4>Comment from original authors:</h4>
                    <p>
                        The demonstration of models that learn from only high-frequency components of the data is
                        an interesting finding that provides us with another way our models can learn from data that
                        appears “meaningless” to humans.
                        The authors fully agree that studying a wider notion of robustness will become increasingly
                        important in ML, and will help us get a better grasp of features we actually want our models
                        to rely on.
                    </p>
                </div>
            </article>

            <article id="response-2">
                <div class=icon></div>
                <header>
                    <h3>
                        <a href="response-2/">Robust Feature Leakage</a>
                    </h3>
                    <div class="authors-affiliations grid">
                        <h3>Authors</h3>
                        <h3>Affiliations</h3>
                        <p class="author">
                            <a class="name" href="https://gabgoh.github.io">Gabriel Goh</a>
                        </p>
                        <p class="affiliation">
                            <a class="affiliation" href="https://openai.com">OpenAI</a>
                        </p>
                    </div>
                </header>
                <p class="summary">
                    Gabriel explores an alternative mechanism that could contribute to the non-robust transfer
                    results.
                    He establishes a lower-bound showing that this mechanism contributes a little bit to the
                    $\widehat{\mathcal{D}}_{rand}$ experiment,
                    but finds no evidence for it effecting the $\widehat{\mathcal{D}}_{det}$ experiment.
                    <a class="continue" href="response-2/">
                        Read Full Article
                    </a>
                </p>
                <div class="rebuttal">
                    <h4>Comment from original authors:</h4>
                    <p>
                        This is a nice in-depth investigation that highlights (and neatly visualizes) one of the
                        motivations for designing the $\widehat{\mathcal{D}}_{det}$ dataset.
                    </p>
                </div>
            </article>

            <article id="response-3">
                <div class=icon></div>
                <header>
                    <h3>
                        <a href="response-3/">Two Examples of Useful, Non-Robust Features</a>
                    </h3>
                    <div class="authors-affiliations grid">
                        <h3>Authors</h3>
                        <h3>Affiliations</h3>
                        <p class="author">
                            <a class="name" href="https://gabgoh.github.io">Gabriel Goh</a>
                        </p>
                        <p class="affiliation">
                            <a class="affiliation" href="https://openai.com">OpenAI</a>
                        </p>
                    </div>
                </header>
                <p class="summary">
                    Gabriel explores what non-robust useful features might look like in the linear case.
                    He provides two constructions:
                    "contaminated" features which are only non-robust due to a non-useful feature being mixed in,
                    and "ensembles" that could be candidates for true useful non-robust features.
                    <a class="continue" href="response-3/">
                        Read Full Article
                    </a>
                </p>
                <div class="rebuttal">
                    <h4>Comment from original authors:</h4>
                    <p>
                        These experiments with linear models are a great first step towards visualizing non-robust
                        features for real datasets (and thus a neat corroboration of their existence).
                        Furthermore, the theoretical construction of "contaminated" non-robust features opens an
                        interesting direction of developing a more fine-grained definition of features.
                    </p>
                </div>
            </article>

            <article id="response-4">
                <div class=icon></div>
                <header>
                    <h3>
                        <a href="response-4/">Adversarially Robust Neural Style Transfer</a>
                    </h3>
                    <div class="authors-affiliations grid">
                        <h3>Authors</h3>
                        <h3></h3>
                        <p class="author">
                            <a class="name" href="https://reiinakano.com/">Reiichiro Nakano</a>
                        </p>
                    </div>
                </header>
                <p class="summary">
                    Reiichiro shows that adversarial robustness makes neural style transfer
                    work by default on a non-VGG architecture.
                    He finds that matching robust features makes style transfer's outputs look perceptually better
                    to humans.
                    <a class="continue" href="response-4/">
                        Read Full Article
                    </a>
                </p>
                <div class="rebuttal">
                    <h4>Comment from original authors:</h4>
                    <p>
                        Very interesting results that highlight the potential role of non-robust features and the
                        utility of robust models for downstream tasks. We're excited to see what kind of impact robustly
                        trained models will have in neural network art!
                        Inspired by these findings, we also take a deeper dive into (non-robust) VGG, and find some
                        interesting links between robustness and style transfer.
                    </p>
                </div>
            </article>

            <article id="response-5">
                <div class=icon></div>
                <header>
                    <h3>
                        <a href="response-5/">Adversarial Examples are Just Bugs, Too</a>
                    </h3>
                    <div class="authors-affiliations grid">
                        <h3>Authors</h3>
                        <h3>Affiliations</h3>
                        <p class="author">
                            <a class="name" href="https://preetum.nakkiran.org/">Preetum Nakkiran</a>
                        </p>
                        <p class="affiliation" style="width: 200px;">
                            <a class="affiliation" href="https://openai.com">OpenAI</a> &
                            <a class="affiliation" href="https://www.harvard.edu/">Harvard University</a>
                        </p>
                    </div>
                </header>
                <p class="summary">
                    Preetum constructs a family of adversarial examples with no transfer to real data,
                    suggesting that some adversarial examples are "bugs" in the original paper's framing.
                    Preetum also demonstrates that adversarial examples can arise even if the underlying distribution
                    has no "non-robust features".
                    <a class="continue" href="response-5/">
                        Read Full Article
                    </a>
                </p>
                <div class="rebuttal">
                    <h4>Comment from original authors:</h4>
                    <p>
                        A fine-grained look at adversarial examples that neatly our thesis (i.e. that non-robust
                        features exist and adversarial examples arise from them, see Takeaway #1) while providing an
                        example of adversarial examples that arise from "bugs".
                        The fact that the constructed "bugs"-based adversarial examples don't transfer constitutes
                        another evidence for the link between transferability and (non-robust) features.
                    </p>
                </div>
            </article>

            <article id="response-6">
                <div class=icon></div>
                <header>
                    <h3>
                        <a href="response-6/">Learning from Incorrectly Labeled Data</a>
                    </h3>
                    <div class="authors-affiliations grid">
                        <h3>Authors</h3>
                        <h3>Affiliations</h3>
                        <p class="author">
                            <a class="name" href="https://www.ericswallace.com/">Eric Wallace</a>
                        </p>
                        <p class="affiliation">
                            <a class="affiliation" href="https://allenai.org/">Allen Institute for AI</a>
                        </p>
                    </div>
                </header>
                <p class="summary">
                    Eric shows that training on a model's training errors,
                    or on how it predicts examples form an unrelated dataset,
                    can both transfer to the true test set.
                    These experiments are analogous to the original paper's non-robust transfer results
                    -- all three results are examples of a kind of "learning from incorrectly labeled data."
                    <a class="continue" href="response-6/">
                        Read Full Article
                    </a>
                </p>
                <div class="rebuttal">
                    <h4>Comment from original authors:</h4>
                    <p>
                        These experiments are a creative demonstration of the fact that the underlying phenomenon of
                        learning features from “human-meaningless” data can actually arise in a broad range of
                        settings.
                    </p>
                </div>
            </article>

        </div>


        <h2 style="margin-bottom: 0">Original Author Discussion and Responses</h2>

        <div class="articles">
            <article id="rebuttal" style="border: none;">
                <div class=icon></div>
                <header>
                    <h3>
                        <a href="original-authors/">Discussion and Author Responses</a>
                    </h3>
                    <div class="authors-affiliations grid">
                        <h3>Authors</h3>
                        <h3>Affiliations</h3>

                        <p class="author" style="line-height: 1.5; margin-top: 4px;">

                            <a class="name" href="http://loganengstrom.com/">Logan Engstrom</a>,
                            <a class="name" href="http://andrewilyas.com/">Andrew Ilyas</a>,
                            <a class="name" href="https://people.csail.mit.edu/madry/">Aleksander Madry</a>,
                            <a class="name" href="http://people.csail.mit.edu/shibani/">Shibani Santurkar</a>,
                            <span class="name">Brandon Tran</span>,
                            <a class="name" href="http://people.csail.mit.edu/tsipras/">Dimitris Tsipras</a>
                        </p>
                        <p class="affiliation">
                            <span class="affiliation">MIT</span>
                        </p>
                    </div>
                </header>
                <p class="summary">
                    The original authors describe their takeaways and some clarifcations that resulted from the
                    conversation.
                    This article also contains their responses to each comment.
                    <a class="continue" href="original-authors/">
                        Read Full Article
                    </a>
                </p>
            </article>
        </div>

    </d-article>

    <d-appendix>
        <h3>Citation Information</h3>
        <p>
            If you wish to cite this discussion as a whole, citation information can be found below.
            The author order is all participants in the conversation in alphabetical order.
            You can also cite individual comments or the author responses using the citation information provided at the
            bottom of the corresponding article.
        </p>

        <h3>Editorial Note</h3>
        <p>
            This discussion article is an experiment organized by Chris Olah and Ludwig Schubert.
            Chris Olah facilitated and edited the comments and discussion process. Ludwig Schubert
            assisted by assembling the responses into their current presentation.
        </p>

        <p>
            We're extremely grateful for the time
            and effort that both the authors of the responses as well as the authors of the original paper put into this
            process, and the patience they had with the editorial team as we experimented with this format.
            Respondents were selected in two ways.
            Some respondents came to our attention because they were actively working on better understanding the Ilyas
            et al results.
            Other respondents were subject matter experts we reached out to.
        </p>

        <p>
            Distill is also grateful to <a href="https://www.inference.vc/">Ferenc Huszár</a> for encouraging us to
            explore this style of article.
        </p>


        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
    </d-appendix>

    <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
